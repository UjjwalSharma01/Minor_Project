"""
XGBoost Classifier - Production Model

This script implements the XGBoost classifier that was chosen as the final model
due to its superior performance and better handling of overfitting through
regularization parameters.
"""

import json
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
import warnings
warnings.filterwarnings('ignore')

class XGBoostNetworkClassifier:
    def __init__(self):
        """
        Initialize XGBoost classifier with regularization to prevent overfitting
        """
        self.model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,  # Limited depth prevents overfitting
            learning_rate=0.1,
            subsample=0.8,  # Row sampling prevents overfitting
            colsample_bytree=0.8,  # Column sampling prevents overfitting
            reg_alpha=0.1,  # L1 regularization
            reg_lambda=1.0,  # L2 regularization
            random_state=42,
            n_jobs=-1
        )
        self.vectorizer = TfidfVectorizer(max_features=1000)
        self.label_mapping = {
            'job_hunting': 0,
            'normal': 1,
            'suspicious': 2
        }
        
    def load_training_data(self, filepath='../Python Code/training_data.json'):
        """Load training data from JSON file"""
        with open(filepath, 'r') as f:
            data = json.load(f)
        
        urls = []
        labels = []
        
        for category, entries in data.items():
            label = self.label_mapping[category]
            for entry in entries:
                urls.append(entry['url'])
                labels.append(label)
        
        return urls, labels
    
    def train(self, urls, labels):
        """Train the XGBoost model"""
        print("=" * 60)
        print("XGBOOST CLASSIFIER - PRODUCTION MODEL")
        print("=" * 60)
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            urls, labels, test_size=0.2, random_state=42, stratify=labels
        )
        
        # Vectorize
        X_train_vec = self.vectorizer.fit_transform(X_train)
        X_test_vec = self.vectorizer.transform(X_test)
        
        print(f"\nðŸ“Š Dataset Split:")
        print(f"   Training samples: {len(X_train)}")
        print(f"   Test samples: {len(X_test)}")
        
        # Train model
        print(f"\nðŸš€ Training XGBoost...")
        print(f"   Parameters:")
        print(f"      max_depth={self.model.max_depth} (controlled)")
        print(f"      subsample={self.model.subsample} (prevents overfitting)")
        print(f"      colsample_bytree={self.model.colsample_bytree}")
        print(f"      reg_alpha={self.model.reg_alpha} (L1 regularization)")
        print(f"      reg_lambda={self.model.reg_lambda} (L2 regularization)")
        
        self.model.fit(X_train_vec, y_train)
        
        # Evaluate on training data
        train_pred = self.model.predict(X_train_vec)
        train_accuracy = accuracy_score(y_train, train_pred)
        
        # Evaluate on test data
        test_pred = self.model.predict(X_test_vec)
        test_accuracy = accuracy_score(y_test, test_pred)
        
        # Calculate overfitting metric
        overfitting_gap = train_accuracy - test_accuracy
        
        print("\n" + "=" * 60)
        print("âœ… GENERALIZATION ANALYSIS")
        print("=" * 60)
        print(f"âœ“ Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)")
        print(f"âœ“ Test Accuracy:     {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
        print(f"âœ“ Overfitting Gap:   {overfitting_gap:.4f} ({overfitting_gap*100:.2f}%)")
        
        if overfitting_gap < 0.05:
            print("\nâœ¨ EXCELLENT GENERALIZATION!")
            print("   Model performs well on both training and unseen test data.")
            print("   Regularization successfully prevents overfitting.")
        
        # Cross-validation scores
        print(f"\nðŸ”„ Cross-Validation Scores (5-fold):")
        cv_scores = cross_val_score(self.model, X_train_vec, y_train, cv=5)
        print(f"   CV Scores: {cv_scores}")
        print(f"   Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
        print(f"   Low variance indicates stable, reliable model")
        
        # Detailed classification report
        print("\n" + "=" * 60)
        print("ðŸ“‹ TEST SET CLASSIFICATION REPORT")
        print("=" * 60)
        print(classification_report(y_test, test_pred, 
                                   target_names=['job_hunting', 'normal', 'suspicious']))
        
        # Confusion matrix
        print("\nðŸ“Š Confusion Matrix (Test Set):")
        cm = confusion_matrix(y_test, test_pred)
        print(cm)
        
        # Feature importance
        print("\nðŸ” Top 10 Most Important Features:")
        feature_names = self.vectorizer.get_feature_names_out()
        importances = self.model.feature_importances_
        indices = np.argsort(importances)[::-1][:10]
        
        for i, idx in enumerate(indices, 1):
            print(f"   {i}. {feature_names[idx]}: {importances[idx]:.4f}")
        
        return {
            'train_accuracy': train_accuracy,
            'test_accuracy': test_accuracy,
            'overfitting_gap': overfitting_gap,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        }
    
    def predict(self, url):
        """Predict category for a single URL"""
        url_vec = self.vectorizer.transform([url])
        prediction = self.model.predict(url_vec)[0]
        probabilities = self.model.predict_proba(url_vec)[0]
        
        reverse_mapping = {v: k for k, v in self.label_mapping.items()}
        
        return {
            'category': reverse_mapping[prediction],
            'probabilities': {
                'job_hunting': float(probabilities[0]),
                'normal': float(probabilities[1]),
                'suspicious': float(probabilities[2])
            }
        }


def main():
    """Main execution function"""
    classifier = XGBoostNetworkClassifier()
    
    # Load training data
    print("ðŸ“ Loading training data...")
    urls, labels = classifier.load_training_data()
    print(f"   Loaded {len(urls)} samples")
    
    # Train and evaluate
    results = classifier.train(urls, labels)
    
    # Test predictions
    print("\n" + "=" * 60)
    print("ðŸ§ª SAMPLE PREDICTIONS")
    print("=" * 60)
    
    test_urls = [
        "linkedin.com/jobs",
        "google.com/search",
        "suspicious-malware-site.com"
    ]
    
    for url in test_urls:
        result = classifier.predict(url)
        print(f"\nURL: {url}")
        print(f"Prediction: {result['category']}")
        print(f"Confidence: {max(result['probabilities'].values())*100:.2f}%")
    
    # Final conclusion
    print("\n" + "=" * 60)
    print("ðŸ’¡ WHY XGBOOST WAS CHOSEN")
    print("=" * 60)
    print("XGBoost demonstrates superior performance:")
    print(f"1. Training accuracy: {results['train_accuracy']*100:.2f}%")
    print(f"2. Test accuracy: {results['test_accuracy']*100:.2f}%")
    print(f"3. Minimal overfitting gap: {results['overfitting_gap']*100:.2f}%")
    print(f"4. Stable cross-validation: {results['cv_mean']*100:.2f}% Â± {results['cv_std']*100:.2f}%")
    print("\nBuilt-in regularization makes it ideal for production use.")
    print("=" * 60)


if __name__ == "__main__":
    main()
