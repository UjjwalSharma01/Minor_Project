# COMPLETE EXPLANATION OF enhanced_classifier.py

---

## **OVERVIEW: What is This File?**

This file contains the **"brain" of your system** - the actual machine learning logic that:
1. Extracts features from DNS logs
2. Trains the XGBoost model
3. Makes predictions
4. Detects anomalies

**Think of it as:**
- `main.py` = The manager (coordinates everything)
- `enhanced_classifier.py` = The worker (does the heavy lifting)

---

## **PART 1: Header and Imports (Lines 1-22)**

```python
#!/usr/bin/env python3
"""
Enhanced Feature Extractor and Classifier with improved accuracy and overfitting prevention
Now integrated with Domain Intelligence for superior categorization
"""
```

**What this means:**
This file is about feature extraction and classification with special domain intelligence.

---

### **Imports and Their Purpose:**

```python
import numpy as np                    # Math operations (calculations, arrays)
import pandas as pd                   # Data manipulation (working with tables)
from sklearn.ensemble import IsolationForest  # Anomaly detection algorithm
from sklearn.model_selection import train_test_split  # Splits data into training/testing
from sklearn.preprocessing import StandardScaler, LabelEncoder  # Data normalization
from sklearn.metrics import classification_report, accuracy_score  # Performance metrics
import xgboost as xgb                 # The XGBoost classifier (main ML algorithm)
import json                           # Read/write JSON files
import logging                        # Write logs
from collections import defaultdict, Counter  # Counting tools
from datetime import datetime         # Time calculations
import joblib                         # Save/load trained models
```

---

## **PART 2: CLASS 1 - EnhancedFeatureExtractor**

This class extracts features from raw DNS logs.

---

### **A. Initialization**

```python
class EnhancedFeatureExtractor:
    def __init__(self, domain_categories_file: str = 'domain_categories.json'):
        self.domain_categories = {}
        self.load_domain_categories(domain_categories_file)
        self.categorizer = self
        self.domain_intelligence = self
```

**What happens when you create this object:**

1. Creates empty dictionary for domain categories
2. Loads the 750 domains from `domain_categories.json`
3. Sets `self.categorizer = self` (so main.py can call it)
4. Sets `self.domain_intelligence = self` (enables advanced features)

**Result:**
```python
self.domain_categories = {
    "youtube.com": "entertainment",
    "github.com": "work",
    "linkedin.com": "unethical",
    ...750 total
}
```

---

### **B. Load Domain Categories**

```python
def load_domain_categories(self, domain_categories_file):
    try:
        with open(domain_categories_file, 'r') as f:
            self.domain_categories = json.load(f)
        logger.info(f"Loaded {len(self.domain_categories)} domain categories")
    except Exception as e:
        logger.error(f"Error loading domain categories: {e}")
        self.domain_categories = {}
```

**What this does:**
- Opens `domain_categories.json`
- Loads it as a Python dictionary
- Logs success (750 domains loaded)
- If error: Logs error and uses empty dictionary

---

### **C. Basic Domain Categorization**

```python
def categorize_domain(self, domain: str, context_domains=None) -> str:
    if not domain:
        return 'neutral'
    
    # Check domain categories first
    if domain in self.domain_categories:
        return self.domain_categories[domain]
    
    # Remove common prefixes for better matching
    domain_lower = domain.lower()
    for prefix in ['www.', 'api.', 'cdn.', 'm.', 'mobile.', 'app.']:
        if domain_lower.startswith(prefix):
            domain_lower = domain_lower[len(prefix):]
            break
```

**What this does:**

**Step 1:** If domain is empty â†’ return 'neutral'

**Step 2:** Look up domain in the loaded dictionary
```python
domain = "youtube.com"
return "entertainment"  # Found!
```

**Step 3:** Clean domain names
```
www.youtube.com â†’ youtube.com
api.github.com â†’ github.com
m.facebook.com â†’ facebook.com
```

**Step 4:** Pattern matching fallback
```python
if any(pattern in domain_lower for pattern in ['facebook', 'instagram', 'youtube', ...]):
    return 'entertainment'
elif any(pattern in domain_lower for pattern in ['github', 'stackoverflow', ...]):
    return 'work'
elif any(pattern in domain_lower for pattern in ['linkedin', 'indeed', 'naukri', 'job']):
    return 'unethical'
```

**Why needed?** For domains not in our 750-domain list, we can still categorize based on keywords.

---

### **D. Enhanced Domain Categorization**

```python
def enhanced_categorize_domain(self, domain: str, context_domains=None):
    """Enhanced domain categorization with context awareness"""
```

**What makes this "enhanced"?**
1. More comprehensive pattern lists
2. Context-aware tracking detection
3. Distinguishes between "pure" content and "tracking"

**Key Innovation: Context-Aware Tracking Attribution**

```python
# If it's a tracking domain, try to attribute it to the parent service
if is_tracking and context_domains:
    entertainment_context = any(
        self.categorize_domain(ctx_domain) == 'entertainment' 
        for ctx_domain in context_domains[:10]
    )
    if entertainment_context:
        category = 'entertainment'

subcategory = "tracking" if is_tracking else "pure"
return category, subcategory
```

**Smart logic:**
```
Scenario:
- User visits youtube.com
- youtube.com loads google-analytics.com (tracking)

Context domains: [youtube.com, google-analytics.com, ...]

For google-analytics.com:
1. Is this tracking? YES
2. Are nearby domains entertainment? YES (youtube.com)
3. Conclusion: Attribute tracking to entertainment

Result: google-analytics.com â†’ ('entertainment', 'tracking')
```

**Why this matters:**
Analytics domains are SUPPORT for the main activity. If someone watches YouTube, YouTube's analytics should count as entertainment, not neutral.

---

### **E. Analyze User Behavior with Intelligence**

```python
def analyze_user_behavior_with_intelligence(self, dns_logs):
    category_counts = {'entertainment': 0, 'work': 0, 'unethical': 0, 'neutral': 0, 'shopping': 0}
    detailed_breakdown = {
        'entertainment': {'pure': 0, 'tracking': 0},
        'work': {'pure': 0, 'tracking': 0},
        ...
    }
```

**What this function does:**
Analyzes ALL DNS logs and provides detailed breakdown with tracking attribution.

**Context Window Concept:**
```
DNS Logs (example):
0: youtube.com
1: youtube.com
2: google-analytics.com  â† Analyzing this (i=2)
3: youtube.com
4: facebook.com

Context window (5 before, 5 after):
Context = [youtube.com, youtube.com, youtube.com, facebook.com]
```

**Result:**
```python
{
    'percentages': {
        'entertainment': 0.70,
        'work': 0.20,
        'unethical': 0.05,
        'neutral': 0.05,
        'shopping': 0.00
    },
    'detailed_breakdown': {
        'entertainment': {'pure': 50, 'tracking': 20},
        'work': {'pure': 18, 'tracking': 2},
    }
}
```

---

### **F. THE MAIN FEATURE EXTRACTION FUNCTION**

```python
def extract_enhanced_features(self, dns_logs, window_minutes=30):
    """Extract enhanced features with domain intelligence integration"""
```

**THIS IS THE MOST IMPORTANT FUNCTION** - Converts raw DNS logs to 23 features.

---

#### **Step 1: Filter Infrastructure Domains**

```python
infrastructure_keywords = [
    'firebase', 'crashlytics', 'googleapis.com', 'play.googleapis',
    'doubleclick', 'googlesyndication', 'googleadservices', 'googletagmanager',
    'app-measurement', 'analytics', 'tracking', 'newrelic', 'branch.io',
    'appsflyer', 'amplitude', 'mixpanel', 'bugsnag', 'sentry.io', 'cdn.'
]

filtered_logs = [
    log for log in dns_logs 
    if not any(kw in log.get('domain', '').lower() for kw in infrastructure_keywords)
]
```

**What this does:**

**Example:**
```
Input logs:
1. youtube.com â†’ Keep (user content)
2. firebase.googleapis.com â†’ REMOVE (infrastructure)
3. github.com â†’ Keep (user content)
4. app-measurement.com â†’ REMOVE (analytics)
5. facebook.com â†’ Keep (user content)

filtered_logs = [youtube.com, github.com, facebook.com]
```

**Why filter?**
We want to analyze **user behavior**, not background app activity.

---

#### **Step 2: Use Domain Intelligence**

```python
intelligence_result = self.domain_intelligence.analyze_user_behavior_with_intelligence(filtered_logs)

entertainment_pct = intelligence_result['percentages']['entertainment']
work_pct = intelligence_result['percentages']['work']
unethical_pct = intelligence_result['percentages']['unethical']

pure_entertainment_pct = detailed_breakdown['entertainment']['pure'] / total_queries
entertainment_tracking_pct = detailed_breakdown['entertainment']['tracking'] / total_queries
```

**Calls intelligence function to get:**
- Category percentages (entertainment, work, etc.)
- Detailed breakdown (pure vs tracking)

---

#### **Step 3: Count Domains and Blocked Queries**

```python
domain_counts = Counter()
blocked_count = 0

for log in dns_logs:
    domain = log.get('domain', '')
    if domain:
        domain_counts[domain] += 1
        
        if log.get('status') == 'blocked':
            blocked_count += 1
```

**Count how many times each domain appears:**
```
youtube.com: 30 times
github.com: 10 times
facebook.com: 15 times

domain_counts = Counter({'youtube.com': 30, 'github.com': 10, 'facebook.com': 15})
```

---

#### **Step 4: Calculate Basic Metrics**

```python
unique_domains = len(domain_counts)
top_domain_concentration = max(domain_counts.values()) / total_queries
blocked_queries_pct = blocked_count / total_queries
```

**Calculates:**

1. **unique_domains**: How many different domains (25)
2. **top_domain_concentration**: Fraction of most visited domain (0.30 = 30%)
3. **blocked_queries_pct**: What percentage was blocked (0.05 = 5%)

---

#### **Step 5: Calculate Temporal Features**

```python
session_duration = self._calculate_session_duration(timestamps)
queries_per_minute = total_queries / max(session_duration, 1)
```

**Calculates:**

1. **session_duration**: Time between first and last query (45 minutes)
2. **queries_per_minute**: Query rate (100 queries / 45 minutes = 2.2/min)

---

#### **Step 6: Calculate Diversity Metrics**

```python
domain_entropy = self._calculate_entropy(list(domain_counts.values()))
category_diversity = len(set([self.categorize_domain(domain) for domain in domain_counts.keys()]))
```

**Calculates:**

1. **domain_entropy**: Shannon entropy (diversity measure)
   - Low entropy (0-1): Focused on few domains
   - High entropy (2-4): Diverse browsing

2. **category_diversity**: How many different categories used (3)

---

#### **Step 7: Calculate Behavioral Patterns**

```python
peak_hour = self._extract_peak_activity_hour(timestamps)
weekend_activity = self._calculate_weekend_activity(timestamps)
```

**Calculates:**

1. **peak_hour**: Which hour had most activity (10 = 10 AM)
2. **weekend_activity**: What fraction was on weekend (0.30 = 30%)

---

#### **Step 8: Calculate Query Patterns**

```python
query_lengths = [len(log.get('domain', '')) for log in dns_logs]
avg_query_length = np.mean(query_lengths)
query_length_variance = np.var(query_lengths)
```

**Domain name lengths:**
```
youtube.com â†’ 11 characters
github.com â†’ 10 characters
facebook.com â†’ 12 characters

avg_query_length = 11
query_length_variance = variance
```

---

#### **Step 9: Calculate Specific Indicators**

```python
social_media_pct = self._calculate_social_media_percentage(dns_logs)
streaming_pct = self._calculate_streaming_percentage(dns_logs)
dev_tools_pct = self._calculate_dev_tools_percentage(dns_logs)
cloud_services_pct = self._calculate_cloud_services_percentage(dns_logs)
```

**Specific percentages:**
- Social media usage (Facebook, Instagram, Twitter)
- Streaming usage (YouTube, Netflix, Spotify)
- Dev tools usage (GitHub, StackOverflow)
- Cloud services usage (AWS, Azure, GCP)

---

#### **Step 10: Return Feature Dictionary**

```python
return {
    # Basic features
    'total_queries': total_queries,
    'unique_domains': unique_domains,
    'entertainment_pct': entertainment_pct,
    'work_pct': work_pct,
    'unethical_pct': unethical_pct,
    'neutral_pct': neutral_pct,
    'shopping_pct': shopping_pct,
    
    # Domain Intelligence features
    'pure_entertainment_pct': pure_entertainment_pct,
    'entertainment_tracking_pct': entertainment_tracking_pct,
    
    # Advanced features
    'session_duration': session_duration,
    'queries_per_minute': queries_per_minute,
    'domain_entropy': domain_entropy,
    'top_domain_concentration': top_domain_concentration,
    'blocked_queries_pct': blocked_queries_pct,
    'category_diversity': category_diversity,
    
    # Temporal features
    'peak_activity_hour': peak_hour,
    'weekend_activity': weekend_activity,
    
    # Query patterns
    'avg_query_length': avg_query_length,
    'query_length_variance': query_length_variance,
    
    # Specific indicators
    'social_media_pct': social_media_pct,
    'streaming_pct': streaming_pct,
    'dev_tools_pct': dev_tools_pct,
    'cloud_services_pct': cloud_services_pct,
    
    # Metadata
    'category_counts': {...},
    'top_domains': {...}
}
```

**Returns complete feature dictionary with 23+ features!**

This is what goes to XGBoost for classification.

---

### **G. Key Helper Functions**

#### **Calculate Entropy (Shannon Entropy)**

```python
def _calculate_entropy(self, values):
    total = sum(values)
    probabilities = [v / total for v in values if v > 0]
    entropy = -sum(p * np.log2(p) for p in probabilities if p > 0)
    return entropy
```

**Formula: Entropy = -Î£(p * log2(p))**

**Example:**
```
Domain counts: [90, 5, 5]  (one dominant domain)
Probabilities: [0.9, 0.05, 0.05]
Entropy = 0.57 (LOW - very focused)

Domain counts: [33, 33, 34]  (balanced)
Probabilities: [0.33, 0.33, 0.34]
Entropy = 1.58 (HIGH - diverse)
```

---

#### **Calculate Session Duration**

```python
def _calculate_session_duration(self, timestamps):
    times = [datetime.fromisoformat(ts.replace('Z', '+00:00')) for ts in timestamps]
    duration = (max(times) - min(times)).total_seconds() / 60
    return max(duration, 1.0)
```

**What this does:**
1. Parse all timestamps to datetime objects
2. Find earliest and latest time
3. Calculate difference in minutes
4. Return (minimum 1 minute)

---

## **PART 3: CLASS 2 - EnhancedBehaviorClassifier**

This class handles the XGBoost model training and prediction.

---

### **A. Initialization**

```python
class EnhancedBehaviorClassifier:
    def __init__(self, training_data_file='training_data.json'):
        self.model = xgb.XGBClassifier(
            max_depth=4,                    # Shallow trees (prevent overfitting)
            min_child_weight=6,             # Min samples per leaf
            n_estimators=100,               # 100 trees
            learning_rate=0.1,              # Conservative learning
            reg_alpha=0.1,                  # L1 regularization
            reg_lambda=1.0,                 # L2 regularization
            subsample=0.8,                  # Use 80% of data per tree
            colsample_bytree=0.8,           # Use 80% of features per tree
            random_state=42,
            n_jobs=-1,                      # Use all CPU cores
            eval_metric='mlogloss',
            objective='multi:softprob'
        )
```

**Key parameters:**

- **max_depth=4**: Trees only 4 levels deep (prevents overfitting)
- **min_child_weight=6**: Each leaf needs â‰¥6 samples (prevents memorization)
- **reg_alpha & reg_lambda**: Regularization penalties (prevent overfitting)
- **subsample=0.8**: Use 80% of data per tree (adds randomness)
- **colsample_bytree=0.8**: Use 80% of features per tree

**All these settings prevent overfitting!**

---

```python
self.scaler = StandardScaler()           # Normalizes features
self.label_encoder = LabelEncoder()      # Converts labels to numbers
self.anomaly_detector = IsolationForest(contamination=0.1, random_state=42)
```

**Three support objects:**

1. **StandardScaler**: Normalizes features (makes them comparable)
2. **LabelEncoder**: "entertainment" â†’ 0, "work" â†’ 1, etc.
3. **IsolationForest**: Anomaly detector (expects 10% anomalies)

---

```python
self.feature_columns = [
    'total_queries', 'unique_domains', 'entertainment_pct', 'work_pct',
    'unethical_pct', 'neutral_pct', 'shopping_pct', 'session_duration',
    'queries_per_minute', 'domain_entropy', 'top_domain_concentration',
    'blocked_queries_pct', 'category_diversity', 'peak_activity_hour',
    'weekend_activity', 'avg_query_length', 'query_length_variance',
    'social_media_pct', 'streaming_pct', 'dev_tools_pct', 'cloud_services_pct',
    'pure_entertainment_pct', 'entertainment_tracking_pct'
]
```

**Lists all 23 feature names** - defines the order.

---

### **B. Training Function**

```python
def train_with_validation(self):
    X, y = self.load_training_data()  # Load 1900 samples
    X_processed = X[self.feature_columns].fillna(0)
```

**Step 1:** Load training data (1900 samples), select 23 features

---

```python
# Split into train/validation/test
X_temp, X_test, y_temp, y_test = train_test_split(
    X_processed, y, test_size=0.25, stratify=y
)

X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.2, stratify=y_temp
)
```

**Step 2:** Three-way split

```
1900 total samples
    â†“
â”œâ”€ 1140 samples (train) â† Model learns from this
â”œâ”€ 285 samples (validation) â† Monitor during training
â””â”€ 475 samples (test) â† Final evaluation
```

**Why three sets?**
- **Train**: Model learns
- **Validation**: Prevents overfitting during training
- **Test**: Unbiased final evaluation

---

```python
# Scale and encode
X_train_scaled = self.scaler.fit_transform(X_train)
X_val_scaled = self.scaler.transform(X_val)
X_test_scaled = self.scaler.transform(X_test)

y_train_encoded = self.label_encoder.fit_transform(y_train)
y_val_encoded = self.label_encoder.transform(y_val)
y_test_encoded = self.label_encoder.transform(y_test)
```

**Step 3:** Normalize features and encode labels

**Important:** `fit_transform` only on train data!

---

```python
# Train with early stopping
self.model.fit(
    X_train_scaled, y_train_encoded,
    eval_set=[(X_train_scaled, y_train_encoded), (X_val_scaled, y_val_encoded)],
    verbose=False
)

# Train anomaly detector
self.anomaly_detector.fit(X_train_scaled)
```

**Step 4:** Train both models

**Early stopping:** If validation accuracy stops improving, training stops automatically.

---

```python
# Evaluate
train_accuracy = self.model.score(X_train_scaled, y_train_encoded)
val_accuracy = self.model.score(X_val_scaled, y_val_encoded)
test_accuracy = accuracy_score(y_test_encoded, y_pred)

# Check overfitting
train_val_gap = train_accuracy - val_accuracy

if train_val_gap > 0.15:
    logger.warning("âš ï¸ HIGH OVERFITTING")
elif train_val_gap > 0.10:
    logger.warning("âš ï¸ MODERATE OVERFITTING")
else:
    logger.info("âœ… Good generalization")
```

**Step 5:** Evaluate and check for overfitting

**Logic:**
```
Train = 100%, Val = 85% â†’ Gap = 15% â†’ HIGH OVERFITTING
Train = 98%, Val = 96% â†’ Gap = 2% â†’ Good!
```

---

### **C. Prediction Function**

```python
def predict_enhanced(self, features):
    # Prepare features
    feature_vector = [features.get(col, 0) for col in self.feature_columns]
    feature_array = np.array(feature_vector).reshape(1, -1)
    feature_scaled = self.scaler.transform(feature_array)
```

**Step 1:** Convert dictionary to array and scale

```
features = {'entertainment_pct': 0.70, 'work_pct': 0.20, ...}
    â†“
feature_vector = [0.70, 0.20, ...]
    â†“
Scale using trained scaler
```

---

```python
# Get prediction
prediction = self.model.predict(feature_scaled)[0]
probabilities = self.model.predict_proba(feature_scaled)[0]

behavior = self.label_encoder.inverse_transform([prediction])[0]
confidence = np.max(probabilities)
```

**Step 2:** Get prediction from model

```
Model outputs:
prediction = 0
probabilities = [0.87, 0.10, 0.02, 0.01, 0.00]

Convert:
behavior = "entertainment"
confidence = 0.87 (87%)
```

---

```python
# Override rules (safety net)
ent_pct = features.get('entertainment_pct', 0)
work_pct = features.get('work_pct', 0)
unethical_pct = features.get('unethical_pct', 0)

if ent_pct > 0.35 and ent_pct > work_pct and ent_pct > unethical_pct:
    behavior = 'entertainment'
elif work_pct > 0.40 and work_pct > ent_pct:
    behavior = 'work'
elif unethical_pct > 0.20:
    behavior = 'unethical'
```

**Step 3:** Post-processing overrides

**Why?** If entertainment is clearly dominant (>35%), force that classification.

---

```python
# Anomaly detection (two methods)
is_anomaly_if = self.anomaly_detector.predict(feature_scaled)[0] == -1
is_anomaly_pattern = self._detect_anomaly(features)

is_anomaly = is_anomaly_if or is_anomaly_pattern

return behavior, confidence, is_anomaly
```

**Step 4:** Check for anomalies using BOTH methods

1. **Isolation Forest** (ML-based)
2. **Pattern rules** (rule-based)

If EITHER flags it â†’ Anomaly!

---

### **D. Pattern-Based Anomaly Detection**

```python
def _detect_anomaly(self, features):
    anomaly_indicators = []
    
    if features.get('entertainment_pct', 0) > 0.8:
        anomaly_indicators.append('Very high entertainment usage')
    
    if features.get('unethical_pct', 0) > 0.3:
        anomaly_indicators.append('High unethical activity')
    
    if features.get('queries_per_minute', 0) > 30:
        anomaly_indicators.append('Unusually high query rate')
    
    if features.get('domain_entropy', 0) < 1.0 and features.get('total_queries', 0) > 100:
        anomaly_indicators.append('Very low domain diversity')
    
    peak_hour = features.get('peak_activity_hour', 12)
    if peak_hour < 6 or peak_hour > 23:
        anomaly_indicators.append('Off-hours peak activity')
    
    return len(anomaly_indicators) > 0
```

**Rule-based checks:**

1. **>80% entertainment** â†’ Flag (excessive)
2. **>30% unethical** â†’ Flag (job hunting)
3. **>30 queries/minute** â†’ Flag (bot-like)
4. **Low entropy + many queries** â†’ Flag (reconnaissance)
5. **Peak at 2 AM** â†’ Flag (suspicious hours)

---

### **E. Save/Load Model**

```python
def save_model(self, filepath='enhanced_behavior_model.pkl'):
    model_data = {
        'model': self.model,
        'scaler': self.scaler,
        'label_encoder': self.label_encoder,
        'anomaly_detector': self.anomaly_detector,
        'feature_columns': self.feature_columns,
        'is_trained': self.is_trained
    }
    joblib.dump(model_data, filepath)
```

**Saves everything** to a `.pkl` file for later use.

```python
def load_model(self, filepath='enhanced_behavior_model.pkl'):
    model_data = joblib.load(filepath)
    self.model = model_data['model']
    self.scaler = model_data['scaler']
    # ... load everything back
```

**Loads saved model** - avoids retraining every time!

---

## **COMPLETE FLOW SUMMARY**

### **TRAINING PHASE:**
```
1. Create EnhancedBehaviorClassifier
2. Load training_data.json (1900 samples)
3. Split into train/validation/test (1140/285/475)
4. Scale features using StandardScaler
5. Encode labels (text â†’ numbers)
6. Train XGBoost model with early stopping
7. Train Isolation Forest (anomaly detector)
8. Evaluate on all three sets
9. Check for overfitting (train-val gap)
10. Save model to disk (.pkl file)
```

### **PREDICTION PHASE:**
```
1. Load real DNS logs
2. Create EnhancedFeatureExtractor
3. Filter infrastructure domains (remove background noise)
4. Categorize each domain using 750-domain dictionary
5. Apply context-aware tracking attribution
6. Calculate 23 features (percentages, entropy, patterns)
7. Scale features using saved scaler
8. Pass to trained XGBoost model
9. Get prediction + confidence
10. Apply override rules (post-processing)
11. Check anomalies (Isolation Forest + patterns)
12. Return behavior + confidence + anomaly flag
```

---

## **KEY CONCEPTS FOR PRESENTATION**

### **What is Feature Extraction?**
> "Feature extraction converts raw DNS logs (domain names and timestamps) into numerical features that machine learning models can understand. We calculate 23 statistical features including category percentages, domain diversity, temporal patterns, and behavioral indicators."

### **What makes it 'Enhanced'?**
> "The enhanced classifier uses domain intelligence with context-aware tracking attribution. When it sees google-analytics.com near youtube.com, it intelligently attributes the analytics tracking to entertainment activity. It also distinguishes between pure content (youtube.com) and support tracking (google-analytics.com)."

### **How does XGBoost work?**
> "XGBoost builds multiple decision trees sequentially, where each tree tries to correct the mistakes of previous trees. We use regularization, subsampling, and early stopping to prevent overfitting. The model learns patterns like 'when entertainment_pct > 0.7 AND work_pct < 0.2, classify as entertainment'."

### **Why two-layer anomaly detection?**
> "We use both Isolation Forest (machine learning-based) and pattern-based rules (domain expertise) for anomaly detection. Isolation Forest learns what 'normal' looks like from training data, while pattern rules catch specific suspicious behaviors like >30 queries/minute or peak activity at 2 AM. If either layer flags it, we mark it as anomalous."

### **What prevents overfitting?**
> "We use multiple techniques: shallow trees (max_depth=4), minimum samples per leaf (min_child_weight=6), L1 and L2 regularization, subsampling (80% data/features per tree), validation set monitoring, and early stopping. We also check the train-validation gap - if >10%, we know we're overfitting."

---

## **TECHNICAL DETAILS FOR DEEP QUESTIONS**

### **Why 23 features?**
More features = more information, but too many = overfitting. We selected 23 features that capture different aspects:
- **Category percentages** (5 features): What they're browsing
- **Temporal patterns** (4 features): When they're active
- **Diversity metrics** (3 features): How varied their browsing is
- **Query patterns** (3 features): Technical characteristics
- **Specific indicators** (6 features): Social media, streaming, dev tools
- **Metadata** (2 features): Pure vs tracking breakdown

### **Why XGBoost over Random Forest?**
XGBoost provides:
- Better accuracy on small datasets
- Built-in regularization (prevents overfitting)
- More control (10+ hyperparameters)
- Sequential learning (trees learn from each other)
- Early stopping capability

### **Why 10% contamination for Isolation Forest?**
Industry standard assumption: ~10% of workplace behavior is anomalous (unusual but not necessarily malicious). This could be developers working odd hours, sales teams with different patterns, or legitimate behavior variations.

### **Why context window of 5 domains?**
Balance between:
- Too small (1-2): Miss context connections
- Too large (20+): Include unrelated activity, slow performance
- Just right (5): Captures immediate context without noise

---

## **FOR YOUR TEACHER**

**Teacher: "What does enhanced_classifier.py do?"**
> "This file contains two main classes: EnhancedFeatureExtractor which converts raw DNS logs into 23 numerical features using domain intelligence and context-aware categorization, and EnhancedBehaviorClassifier which trains the XGBoost model on 1900 samples and makes predictions with two-layer anomaly detection. It's the core machine learning engine of the system."

**Teacher: "How does feature extraction work?"**
> "We filter out infrastructure domains first, then categorize each remaining domain using our 750-domain dictionary with context-aware tracking attribution. We calculate 23 features across five categories: category percentages, temporal patterns, diversity metrics, query characteristics, and specific behavioral indicators. These numerical features represent user behavior in a format XGBoost can process."

**Teacher: "How does the classifier prevent overfitting?"**
> "We use a multi-pronged approach: XGBoost is configured with shallow trees (depth=4), minimum leaf samples (6), L1 and L2 regularization, and only uses 80% of data and features per tree. During training, we split into train/validation/test sets, monitor the validation set, use early stopping, and check the train-validation accuracy gap. If the gap exceeds 10%, we know we're overfitting."

**Teacher: "Why two-layer anomaly detection?"**
> "Layer 1 is Isolation Forest - it learns statistically what 'normal' workplace behavior looks like from our training data. Layer 2 is rule-based pattern detection using domain expertise to catch specific suspicious activities like excessive entertainment (>80%), high unethical activity (>30%), bot-like query rates (>30/min), reconnaissance patterns (low diversity + high volume), or off-hours activity (peak before 6 AM or after 11 PM). This combination catches both statistical outliers and known suspicious patterns."

---

## **QUICK REFERENCE: KEY NUMBERS**

- **750 domains** in domain_categories.json
- **1900 training samples** in training_data.json
- **23 features** extracted from DNS logs
- **3-way split**: 60% train, 15% validation, 25% test
- **10% contamination** in Isolation Forest
- **5-domain context** window for tracking attribution
- **4 levels max** tree depth in XGBoost
- **80% subsampling** of data and features
- **100 trees** in XGBoost ensemble

---

## **FINAL SUMMARY**

This file is the **heart** of your ML system:

1. **EnhancedFeatureExtractor** - Transforms raw DNS logs into 23 meaningful numerical features using domain intelligence
2. **EnhancedBehaviorClassifier** - Trains XGBoost on 1900 samples with overfitting prevention, makes predictions, and detects anomalies

Together, they convert network logs into actionable behavioral insights with high accuracy and reliability.

**Good luck with your presentation! ðŸŽ¯**
